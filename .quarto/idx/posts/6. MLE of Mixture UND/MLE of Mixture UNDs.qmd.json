{"title":"Maximum Likelihood Estimation of Mixture Univariate Normal Distributions","markdown":{"yaml":{"title":"Maximum Likelihood Estimation of Mixture Univariate Normal Distributions","author":"Clavecillas, Pearllyn","date":"2024-09-15","categories":["Definition"],"image":"image.jpg"},"containsRefs":false,"markdown":"\n\n**MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION**\n\n**Gaussian Mixture Model (GMM)**\n\nMLE of Gaussian Mixture Model (GMM)\n\nGaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.\n\nFor a Gaussian Mixture Model (GMM) with $K$ components, the probability density function (pdf) of a data point $x$ is:\n\n$$\np(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n$$\n\nwhere:\n\n$\\pi_k$ - is the mixing weight of the $k$-th component, with $\\pi_k\\geq0$ and $(\\sum_{k=1}^K \\pi_k=1)$ .\n\n$\\mathcal{N}(x|\\mu_k, \\Sigma_k))$ - is the Gaussian pdf for the $k$-th component:\n\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n$$\n\n$K$ is the total number of Gaussian Components.\n\nNote:\n\n$\\mu_k$ - is the mean vector of the $k$ - th Gaussian.\n\n$\\sum_k$ - is the covariance matrix of the $k$ - th Gaussian\n\nLikelihood Function\n\nGiven a dataset ${x_1,x_2,\\ldots,x_n}$, the likelihood function $\\mathcal{L}(\\theta)$ is the joint probability of observing the entire dataset:\n\n$$\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N p(x_i|\\Theta)\n$$\n\nSubstituting the pdf of the GMM we have:\n\n$$\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n\nLog-Likelihood Function\n\nTo simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:\n\n$$\n\\mathcal{L}(\\Theta) = \\log \\left( \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right) \\right)\n$$\n\nUsing the property of logarithms $(\\log(ab)=\\log(a)+\\log(b))$:\n\n$$\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n\nExpectation-Maximization (EM) Algorithm\n\nTo find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:\n\nE-Step (Expectation Step)\n\nThe E-step computes the responsibility $\\gamma_{ik}$​, which is the probability that data point $\\mathbf{x}_k$​ was generated by the $k$-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:\n\n$$\n\\gamma_{ik} = P(z_i = k|\\mathbf{x}_i)\n$$\n\nwhere:\n\n$z_i$ is the latent variable indicating the component responsible for generating $\\mathbf{x}_i$,\n\n$P(z_i=k|\\mathbf{x}_i)$ is the posterior probability that component $k$ is responsible for $\\mathbf{x}_i$​.\n\nBy Bayes' theorem, the responsibility $\\gamma_{ik}$​ can be written as:\n\n$$\n\\gamma_{ik} =\\frac{P(z_i=k)P(\\mathbf{x}_i|z_i=k)}{P(\\mathbf{x}_i)}\n$$​\n\nWhere:\n\n$P(z_i=k)=\\pi_k$ is the prior probability of selecting the kkk-th component (the mixing weight),\n\n$P(\\mathbf{x}i|z_i=k)=\\mathcal{N}(\\mathbf{x}i|\\mu_k, \\Sigma_k)$ is the likelihood of the data point $\\mathbf{x}_i$ under component $k$,\n\n$P(\\mathbf{x}_i)$ is the marginal likelihood, or the total probability of observing $\\mathbf{x}_i$​, which is a weighted sum of the likelihoods over all components.\n\nThen expressing the marginal likelihood $P(\\mathbf{x}_i)$ as:\n\n$$\nP(\\mathbf{x}_i) = \\Sigma_{j=1}^K P(z_i=j)P(\\mathbf{x}_i|z_i=j)\n$$\n\nand the Gaussian likelihoods, we get:\n\n$$\nP(\\mathbf{x}_i)=\\Sigma_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}_i|\\mu_j, \\Sigma_j)\n$$\n\nNow, we substitute the expressions for $P(z_i=k), P(\\mathbf{x}i|zi=k)$ , and $P(\\mathbf{x}_i)$ into the equation for $\\gamma_{ik}$​:\n\n$$\n\\gamma_{ik} =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_i|\\mu_k, \\Sigma_k)}{\\Sigma_{j=1}^K\\mathcal{N}(\\mathbf{x}_i|\\mu_i,\\Sigma_j)}\n$$\n\nHence, this is the responsibility $\\gamma_{ik}$​, which represents the posterior probability that data point $\\mathbf{x}_i$​ was generated by component $k$.\n\nM-Step (Maximization Step)\n\nUpdating the parameters $(\\pi_k)$, $(\\mu_k)$, and $(\\Sigma_k)$ using the responsibilities $(\\gamma_{ik})$ :\n\nUpdate Mixing Weights\n\nThe mixing weight $(\\pi_k)$ is updated as:\n\n$$\n\\pi_k = \\frac{1}{N} \\sum_{i=1}^N \\gamma_{ik}\n$$\n\nIn a GMM, the data is assumed to be generated from a mixture of $K$ Gaussian distributions. Each Gaussian component $k$ has an associated mixing weight $\\pi_k$​. These weights determine how much each Gaussian component contributes to the overall mixture model.\n\nMathematical Representation: The mixing weight $\\pi_k$ is a non-negative value that satisfies:\n\n$$\n\\pi_k\\geq0\n$$\n\nand\n\n$$\n\\sum_{k-1}^K \\pi_k=1\n$$\n\nThis ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.\n\nUpdate Means\n\nThe mean $(\\mu_k)$ is updated as:\n\n$$\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}}\n$$\n\nwhere:\n\n$\\gamma_{ik}$ is the responsibility of component $k$ for data point $x_i$​, calculated in the E-step.\n\nThe numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component $k$.\n\nUpdate Covariances\n\nThe covariance $(\\Sigma_k)$ is updated as:\n\n$$\n\\Sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k) (x_i - \\mu_k)^T}{\\sum_{i=1}^N \\gamma_{ik}}\n$$\n\nWhere:\n\n$\\gamma_{ik}$ is the responsibility that component $k$ has for data point $x_i$​, calculated in the E-step.\n\n$(x_i - \\mu_k)$ is the difference between the data point and the mean of the kkk-th component.\n\nThe numerator represents the weighted sum of the outer products of the difference vectors $(x_i - \\mu_k)$, which captures both the variance and covariance of the data assigned to the $k$-th component.\n\nThe denominator is the sum of the responsibilities for component $k$, which normalizes the weighted sum.\n\nIn conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clustering and more flexible modeling of complex data distributions.\n","srcMarkdownNoYaml":"\n\n**MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION**\n\n**Gaussian Mixture Model (GMM)**\n\nMLE of Gaussian Mixture Model (GMM)\n\nGaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.\n\nFor a Gaussian Mixture Model (GMM) with $K$ components, the probability density function (pdf) of a data point $x$ is:\n\n$$\np(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n$$\n\nwhere:\n\n$\\pi_k$ - is the mixing weight of the $k$-th component, with $\\pi_k\\geq0$ and $(\\sum_{k=1}^K \\pi_k=1)$ .\n\n$\\mathcal{N}(x|\\mu_k, \\Sigma_k))$ - is the Gaussian pdf for the $k$-th component:\n\n$$\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n$$\n\n$K$ is the total number of Gaussian Components.\n\nNote:\n\n$\\mu_k$ - is the mean vector of the $k$ - th Gaussian.\n\n$\\sum_k$ - is the covariance matrix of the $k$ - th Gaussian\n\nLikelihood Function\n\nGiven a dataset ${x_1,x_2,\\ldots,x_n}$, the likelihood function $\\mathcal{L}(\\theta)$ is the joint probability of observing the entire dataset:\n\n$$\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N p(x_i|\\Theta)\n$$\n\nSubstituting the pdf of the GMM we have:\n\n$$\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n\nLog-Likelihood Function\n\nTo simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:\n\n$$\n\\mathcal{L}(\\Theta) = \\log \\left( \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right) \\right)\n$$\n\nUsing the property of logarithms $(\\log(ab)=\\log(a)+\\log(b))$:\n\n$$\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n\nExpectation-Maximization (EM) Algorithm\n\nTo find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:\n\nE-Step (Expectation Step)\n\nThe E-step computes the responsibility $\\gamma_{ik}$​, which is the probability that data point $\\mathbf{x}_k$​ was generated by the $k$-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:\n\n$$\n\\gamma_{ik} = P(z_i = k|\\mathbf{x}_i)\n$$\n\nwhere:\n\n$z_i$ is the latent variable indicating the component responsible for generating $\\mathbf{x}_i$,\n\n$P(z_i=k|\\mathbf{x}_i)$ is the posterior probability that component $k$ is responsible for $\\mathbf{x}_i$​.\n\nBy Bayes' theorem, the responsibility $\\gamma_{ik}$​ can be written as:\n\n$$\n\\gamma_{ik} =\\frac{P(z_i=k)P(\\mathbf{x}_i|z_i=k)}{P(\\mathbf{x}_i)}\n$$​\n\nWhere:\n\n$P(z_i=k)=\\pi_k$ is the prior probability of selecting the kkk-th component (the mixing weight),\n\n$P(\\mathbf{x}i|z_i=k)=\\mathcal{N}(\\mathbf{x}i|\\mu_k, \\Sigma_k)$ is the likelihood of the data point $\\mathbf{x}_i$ under component $k$,\n\n$P(\\mathbf{x}_i)$ is the marginal likelihood, or the total probability of observing $\\mathbf{x}_i$​, which is a weighted sum of the likelihoods over all components.\n\nThen expressing the marginal likelihood $P(\\mathbf{x}_i)$ as:\n\n$$\nP(\\mathbf{x}_i) = \\Sigma_{j=1}^K P(z_i=j)P(\\mathbf{x}_i|z_i=j)\n$$\n\nand the Gaussian likelihoods, we get:\n\n$$\nP(\\mathbf{x}_i)=\\Sigma_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}_i|\\mu_j, \\Sigma_j)\n$$\n\nNow, we substitute the expressions for $P(z_i=k), P(\\mathbf{x}i|zi=k)$ , and $P(\\mathbf{x}_i)$ into the equation for $\\gamma_{ik}$​:\n\n$$\n\\gamma_{ik} =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_i|\\mu_k, \\Sigma_k)}{\\Sigma_{j=1}^K\\mathcal{N}(\\mathbf{x}_i|\\mu_i,\\Sigma_j)}\n$$\n\nHence, this is the responsibility $\\gamma_{ik}$​, which represents the posterior probability that data point $\\mathbf{x}_i$​ was generated by component $k$.\n\nM-Step (Maximization Step)\n\nUpdating the parameters $(\\pi_k)$, $(\\mu_k)$, and $(\\Sigma_k)$ using the responsibilities $(\\gamma_{ik})$ :\n\nUpdate Mixing Weights\n\nThe mixing weight $(\\pi_k)$ is updated as:\n\n$$\n\\pi_k = \\frac{1}{N} \\sum_{i=1}^N \\gamma_{ik}\n$$\n\nIn a GMM, the data is assumed to be generated from a mixture of $K$ Gaussian distributions. Each Gaussian component $k$ has an associated mixing weight $\\pi_k$​. These weights determine how much each Gaussian component contributes to the overall mixture model.\n\nMathematical Representation: The mixing weight $\\pi_k$ is a non-negative value that satisfies:\n\n$$\n\\pi_k\\geq0\n$$\n\nand\n\n$$\n\\sum_{k-1}^K \\pi_k=1\n$$\n\nThis ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.\n\nUpdate Means\n\nThe mean $(\\mu_k)$ is updated as:\n\n$$\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}}\n$$\n\nwhere:\n\n$\\gamma_{ik}$ is the responsibility of component $k$ for data point $x_i$​, calculated in the E-step.\n\nThe numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component $k$.\n\nUpdate Covariances\n\nThe covariance $(\\Sigma_k)$ is updated as:\n\n$$\n\\Sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k) (x_i - \\mu_k)^T}{\\sum_{i=1}^N \\gamma_{ik}}\n$$\n\nWhere:\n\n$\\gamma_{ik}$ is the responsibility that component $k$ has for data point $x_i$​, calculated in the E-step.\n\n$(x_i - \\mu_k)$ is the difference between the data point and the mean of the kkk-th component.\n\nThe numerator represents the weighted sum of the outer products of the difference vectors $(x_i - \\mu_k)$, which captures both the variance and covariance of the data assigned to the $k$-th component.\n\nThe denominator is the sum of the responsibilities for component $k$, which normalizes the weighted sum.\n\nIn conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clustering and more flexible modeling of complex data distributions.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"MLE of Mixture UNDs.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.553","theme":"cosmo","title-block-banner":true,"title":"Maximum Likelihood Estimation of Mixture Univariate Normal Distributions","author":"Clavecillas, Pearllyn","date":"2024-09-15","categories":["Definition"],"image":"image.jpg"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}