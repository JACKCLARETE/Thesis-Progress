[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Thesis-Progress",
    "section": "",
    "text": "Welcome To Our Thesis Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nClarete, Clavecillas, Salera, Saligumba\n\n\n\n\n\n\n\n\n\n\n\n\nPreliminary Concepts\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nClarete, Clavecillas, Salera, Saligumba\n\n\n\n\n\n\n\n\n\n\n\n\nUnivariate Normal Distributions\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nSalera, Keen Hart D.\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimations of Univariate Normal Distributions\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nClarete, Jack C.\n\n\n\n\n\n\n\n\n\n\n\n\nMixture of Univariate Normal Distributions (Gaussian Mixture Models)\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nKeziah M. Saligumba\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum Likelihood Estimation of Mixture Univariate Normal Distributions\n\n\n\n\n\n\nDefinition\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nClavecillas, Pearllyn\n\n\n\n\n\n\n\n\n\n\n\n\nThesis Proper\n\n\n\n\n\n\nPaper\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nClarete, Clavecillas, Salera, Saligumba\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/4. MLE of UND/MLE of UNDs.html",
    "href": "posts/4. MLE of UND/MLE of UNDs.html",
    "title": "Maximum Likelihood Estimations of Univariate Normal Distributions",
    "section": "",
    "text": "Assume that you have a random sample \\(X_1, ... , X_n\\) from a probability distribution with a probability density function (PDF) or probability mass function (PMF) \\(f(X_i,\\theta)\\), where \\(\\theta\\) is the parameter (or set of parameters) to be estimated.\nThe likelihood function \\(L(\\theta)\\) is the joint probability (or joint density) of observing the sample data, given the parameter \\(\\theta\\). For independent and identically distributed (i.i.d.) data, the likelihood function is the product of the individual densities:\n\\[\nL(\\theta) = \\prod_{i=1}^n f(X_i|\\theta)\n\\]\n\n\n\nSince the likelihood function is a product, it can be difficult to work with directly. Taking the natural logarithm of the likelihood (called the log-likelihood) simplifies things. The log-likelihood function is:\n\\[\n\\ell(\\theta) = logL(\\theta) = \\sum_{i=1}^n f(X_i|\\theta)\n\\]\n\n\n\nTo find the MLE, you need to maximize the log-likelihood function with respect to the parameter \\(\\theta\\). To do this, take the derivative of the log-likelihood function with respect to \\(\\theta\\), and set it equal to zero:\n\\[\n\\frac{d}{d\\theta}\\ell(\\theta) = 0\n\\] this will give you an equation in \\(\\theta\\)\n\n\n\nSolve the equation obtained in Step 3 to get the MLE, \\(\\hat{\\theta}\\). This is the parameter value that maximizes the likelihood of observing the given data\nTo ensure that the value you found is indeed a maximum, you can take the second derivative of the log-likelihood function with respect to \\(\\theta\\) and check if it’s negative: \\[\n\\frac{d^2}{d\\theta^2}\\ell(\\theta)&lt;0\n\\] If the second derivative is negative, you have found a maximum"
  },
  {
    "objectID": "posts/4. MLE of UND/MLE of UNDs.html#steps-in-finding-the-maximum-likelihood-estimation-of-a-distribution",
    "href": "posts/4. MLE of UND/MLE of UNDs.html#steps-in-finding-the-maximum-likelihood-estimation-of-a-distribution",
    "title": "Maximum Likelihood Estimations of Univariate Normal Distributions",
    "section": "",
    "text": "Assume that you have a random sample \\(X_1, ... , X_n\\) from a probability distribution with a probability density function (PDF) or probability mass function (PMF) \\(f(X_i,\\theta)\\), where \\(\\theta\\) is the parameter (or set of parameters) to be estimated.\nThe likelihood function \\(L(\\theta)\\) is the joint probability (or joint density) of observing the sample data, given the parameter \\(\\theta\\). For independent and identically distributed (i.i.d.) data, the likelihood function is the product of the individual densities:\n\\[\nL(\\theta) = \\prod_{i=1}^n f(X_i|\\theta)\n\\]\n\n\n\nSince the likelihood function is a product, it can be difficult to work with directly. Taking the natural logarithm of the likelihood (called the log-likelihood) simplifies things. The log-likelihood function is:\n\\[\n\\ell(\\theta) = logL(\\theta) = \\sum_{i=1}^n f(X_i|\\theta)\n\\]\n\n\n\nTo find the MLE, you need to maximize the log-likelihood function with respect to the parameter \\(\\theta\\). To do this, take the derivative of the log-likelihood function with respect to \\(\\theta\\), and set it equal to zero:\n\\[\n\\frac{d}{d\\theta}\\ell(\\theta) = 0\n\\] this will give you an equation in \\(\\theta\\)\n\n\n\nSolve the equation obtained in Step 3 to get the MLE, \\(\\hat{\\theta}\\). This is the parameter value that maximizes the likelihood of observing the given data\nTo ensure that the value you found is indeed a maximum, you can take the second derivative of the log-likelihood function with respect to \\(\\theta\\) and check if it’s negative: \\[\n\\frac{d^2}{d\\theta^2}\\ell(\\theta)&lt;0\n\\] If the second derivative is negative, you have found a maximum"
  },
  {
    "objectID": "posts/4. MLE of UND/MLE of UNDs.html#maximum-likelihood-estimation-mle-of-normal-distribution",
    "href": "posts/4. MLE of UND/MLE of UNDs.html#maximum-likelihood-estimation-mle-of-normal-distribution",
    "title": "Maximum Likelihood Estimations of Univariate Normal Distributions",
    "section": "Maximum Likelihood Estimation (MLE) of Normal Distribution",
    "text": "Maximum Likelihood Estimation (MLE) of Normal Distribution\nSuppose we have \\(n\\) observations \\(X_1, ..., X_n\\) from a Gaussian (Normal) Distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\). To find the Maximum Likelihood Estimate (MLE) for \\(\\mu\\), we find the log-likelihood \\(\\ell(\\mu)\\) , take the derivative with respect to \\(\\mu\\), then set it equal to zero, and solve for \\(\\mu\\) .\n\\[\nL(\\mu) = \\prod_{i-1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}}exp\\{\\frac{1}{2}(\\frac{x-\\mu}{\\sigma})^2\\}\n\\]\n\\[\n\\Rightarrow \\ell(\\mu) = \\sum_{i=1}^n [log(\\frac{1}{\\sqrt{2\\pi\\sigma^2}})-\\frac{(x_i-\\mu)^2}{2\\sigma^2}]\n\\]\n\\[\n\\Rightarrow \\frac{d}{d\\mu}\\ell(\\mu) = \\sum_{i=1}^n \\frac{x_i-\\mu}{\\sigma^2}\n\\]\nSetting this equal to zero and solving for \\(\\mu\\), we get that \\(\\mu_{MLE} = \\frac{1}{n}\\sum_{i=1}^nx_i\\) .\nNote that applying the log function to the likelihood helped us decompose the product and removed the exponential function so that we could easily solve for the MLE.\nAcknowledgement:\nReferences:"
  },
  {
    "objectID": "posts/5. Mixture of UND (GMM)/Mixture of UND(GMM).html",
    "href": "posts/5. Mixture of UND (GMM)/Mixture of UND(GMM).html",
    "title": "Mixture of Univariate Normal Distributions (Gaussian Mixture Models)",
    "section": "",
    "text": "Gaussian Mixtures (Titterington et al., 1985) are ubiquitous probabilistic models for density estimation in machine learning applications.\nClustering is the process of organizing similar data points into groups, revealing patterns and insights that may not be immediately apparent. It divides data into distinct categories based on behavior, providing a clearer perspective of how the data is arranged. However, what happens when some data points overlap between clusters or do not clearly belong to just one category?"
  },
  {
    "objectID": "posts/5. Mixture of UND (GMM)/Mixture of UND(GMM).html#definition",
    "href": "posts/5. Mixture of UND (GMM)/Mixture of UND(GMM).html#definition",
    "title": "Mixture of Univariate Normal Distributions (Gaussian Mixture Models)",
    "section": "Definition",
    "text": "Definition\nIn a GMM, each component follows a Gaussian distribution, which makes it useful when data comes from multiple overlapping clusters.\n\n\n\nFig. 1: Gaussian Mixture Model\n\n\nIn a univariate normal distribution, the data is from a single dimension, such as height or weight. It is characterized by two key parameters:\n\\(\\bullet\\) Mean (\\(\\mu\\)): which is the average value, represents the central value where data points are likely to be found\n\\(\\bullet\\) Variance(\\(\\sigma^2\\)): where it determines the spread of the data points around the mean\nThe Gaussian Distribution has a density function of:\n\\[f(x) = f_x(x;\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{-(x-\\mu)^2/2\\sigma^2}\\]\nIn the context of Gaussian Mixture Model, assume that we have a random variables of \\(X_1\\), …, \\(X_n\\), and that each \\(X_i\\) is sampled from one of the \\(K\\) mixture components. Associated with each random variable \\(X_i\\) is \\(Z_i \\in \\{1,\\ldots,K\\}\\) which \\(X_i\\) is from. Usually, we do not observe \\(Z_i\\) which are our latent variables.\nFrom the law of total probability,\n\\[P(A) = \\sum P(A \\cap B_k)\\] the marginal probability of \\(X_i\\) is:\n\\[P(X_i = x) = \\sum_{k=1}^K P(X_i=x|Z_i=k)\\underbrace{P(Z_i=k)}_{\\pi_k} = \\sum_{k=1}^K P(X_i=x|Z_i=k)\\pi_k\\]\nwhere \\(\\pi_k\\) are called mixture weights and represent the probability that \\(X_i\\) belongs to the \\(k\\)-th mixture components. As said above, mixture weights must sum to 1 where \\(\\sum_{k=1}^K \\pi_k =1\\). The mixture component is represented as \\(P(X_i |Z_i = k)\\).\nThe Probability Mass Function for discrete random variables of this mixture model is:\n\\[p(x) =  \\sum_{k=1}^{K}\\pi_k p(x \\mid Z_{k})\\]\nor could also be written as\n\\[p(x) =  \\sum_{k=1}^{K}\\pi_k \\cdot f(x \\mid \\mu_k, \\sigma_k^2)\\] For multiple dimensions, the probability density function for continuous random variables of the mixture model is:\n\\[f_{x}(x) = \\sum_{k=1}^{K}\\pi_k f_{x \\mid Z_{k}}(x \\mid Z_{k}) \\] or could also be written as\n\\[p(x) = \\sum_{k=1}^{K} \\pi_k \\cdot f(x \\mid \\mu_k, \\Sigma_k)\\] where \\(f(x \\mid \\mu_k, \\Sigma_k)\\) mal distribution with mean vector \\(\\mu_k\\) and covariance matrix \\(\\sum_k\\)."
  },
  {
    "objectID": "posts/5. Mixture of UND (GMM)/Mixture of UND(GMM).html#visualizing-gmm",
    "href": "posts/5. Mixture of UND (GMM)/Mixture of UND(GMM).html#visualizing-gmm",
    "title": "Mixture of Univariate Normal Distributions (Gaussian Mixture Models)",
    "section": "Visualizing GMM",
    "text": "Visualizing GMM\nNow assume our data are the heights of students at the University of Chicago. Assume the height of a randomly chosen male is normally distributed with a mean equal to 5′9 and a standard deviation of 2.5 inches and the height of a randomly chosen female is \\(N(5′4,2.5)\\).However, instead of 50/50 mixture proportions, assume that 75% of the population is female, and 25% is male. We simulate heights in a similar fashion as above, with the corresponding changes to the parameters:\n\nNUM.SAMPLES &lt;- 5000\nheights      &lt;- numeric(NUM.SAMPLES)\nfor(i in seq_len(NUM.SAMPLES)) {\n  z.i &lt;- rbinom(1,1,0.75)\n  if(z.i == 0) heights[i] &lt;- rnorm(1, mean = 69, sd = 2.5)\n  else heights[i] &lt;- rnorm(1, mean = 64, sd = 2.5)\n}\nhist(heights)\n\n\n\n\n\n\n\n\nNow we see that histogram is unimodal. Are heights normally distributed under this model? We plot the corresponding densities below:\n\n\n\n\n\n\n\n\n\nHere we see that the Gaussian mixture model is unimodal because there is so much overlap between the two densities. In this example, you can see that the population density is not symmetric, and therefore not normally distributed.\nThese two illustrative examples above give rise to the general notion of a mixture model which assumes each observation is generated from one of \\(K\\) mixture components.\nAcknowledgement: The “Examples” section above was taken from lecture notes written by Ramesh Sridharan.\nReferences: Bernstein, M., Stephens, M. (Github Repositories)"
  },
  {
    "objectID": "posts/2. preliminary concepts/Preliminary Concepts.html",
    "href": "posts/2. preliminary concepts/Preliminary Concepts.html",
    "title": "Preliminary Concepts",
    "section": "",
    "text": "The following concepts included in here are the concepts that will be used in this Thesis Blog."
  },
  {
    "objectID": "posts/2. preliminary concepts/Preliminary Concepts.html#parametric-family-of-distributions",
    "href": "posts/2. preliminary concepts/Preliminary Concepts.html#parametric-family-of-distributions",
    "title": "Preliminary Concepts",
    "section": "1. Parametric Family of Distributions",
    "text": "1. Parametric Family of Distributions\nThe Parametric Family of Distributions are divided into 2 categories: Discrete and Continuous.\n\nDiscrete Distributions\n\nDiscrete Uniform Distributions\nBernoulli Distribution\nBinomial Distribution\nHypergeometric Distribution\nPoisson Distribution\nGeometric Distribution\nNegative Binomial Distribution\n\n\n\nContinuous Distributions\n\nUniform Distribution\nNormal Distribution\nGamma Distribution\nExponential Distribution\nBeta Distribution\nCauchy Distribution\nLognormal Distribution"
  },
  {
    "objectID": "posts/2. preliminary concepts/Preliminary Concepts.html#probability-mass-function-pmf-and-probability-density-function-pdf",
    "href": "posts/2. preliminary concepts/Preliminary Concepts.html#probability-mass-function-pmf-and-probability-density-function-pdf",
    "title": "Preliminary Concepts",
    "section": "2. Probability Mass Function (PMF) and Probability Density Function (PDF)",
    "text": "2. Probability Mass Function (PMF) and Probability Density Function (PDF)"
  },
  {
    "objectID": "posts/2. preliminary concepts/Preliminary Concepts.html#maximum-likelihood-estimations-mle",
    "href": "posts/2. preliminary concepts/Preliminary Concepts.html#maximum-likelihood-estimations-mle",
    "title": "Preliminary Concepts",
    "section": "3. Maximum Likelihood Estimations (MLE)",
    "text": "3. Maximum Likelihood Estimations (MLE)"
  },
  {
    "objectID": "posts/2. preliminary concepts/Preliminary Concepts.html#estimation-maximization",
    "href": "posts/2. preliminary concepts/Preliminary Concepts.html#estimation-maximization",
    "title": "Preliminary Concepts",
    "section": "4. Estimation Maximization",
    "text": "4. Estimation Maximization"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/1. welcome/welcome.html",
    "href": "posts/1. welcome/welcome.html",
    "title": "Welcome To Our Thesis Blog",
    "section": "",
    "text": "Welcome to our Thesis Blog, where we document the progress and contents of our Final requirement for this program: Bachelor of Science in Statistics\nWe are composed of 4 members, namely:"
  },
  {
    "objectID": "posts/1. welcome/welcome.html#jack-ceriaca-clarete",
    "href": "posts/1. welcome/welcome.html#jack-ceriaca-clarete",
    "title": "Welcome To Our Thesis Blog",
    "section": "1. Jack Ceriaca Clarete",
    "text": "1. Jack Ceriaca Clarete"
  },
  {
    "objectID": "posts/1. welcome/welcome.html#pearlyn-clavecillas",
    "href": "posts/1. welcome/welcome.html#pearlyn-clavecillas",
    "title": "Welcome To Our Thesis Blog",
    "section": "2. Pearlyn Clavecillas",
    "text": "2. Pearlyn Clavecillas"
  },
  {
    "objectID": "posts/1. welcome/welcome.html#keen-hart-demecillo-salera",
    "href": "posts/1. welcome/welcome.html#keen-hart-demecillo-salera",
    "title": "Welcome To Our Thesis Blog",
    "section": "3. Keen Hart Demecillo Salera",
    "text": "3. Keen Hart Demecillo Salera"
  },
  {
    "objectID": "posts/1. welcome/welcome.html#keziah-saligumba",
    "href": "posts/1. welcome/welcome.html#keziah-saligumba",
    "title": "Welcome To Our Thesis Blog",
    "section": "4. Keziah Saligumba",
    "text": "4. Keziah Saligumba"
  },
  {
    "objectID": "posts/3. UND/UND.html",
    "href": "posts/3. UND/UND.html",
    "title": "Univariate Normal Distributions",
    "section": "",
    "text": "Abraham de Moivre, an century statistician and consultant to gamblers, was often called upon to make these lengthy computations. de Moivre noted that when the number of events (coin flips) increased, the shape of the binomial distribution approached a very smooth curve.de Moivre reasoned that if he could find a mathematical expression for this curve, he would be able to solve problems such as finding the probability of or more heads out of coin flips much more easily. This is exactly what he did, and the curve he discovered is now called the “normal curve.”\nOne of the first applications of the normal distribution was to the analysis of errors of measurement made in astronomical observations, errors that occurred because of imperfect instruments and imperfect observers. Galileo in the century noted that these errors were symmetric and that small errors occurred more frequently than large errors. This led to several hypothesized distributions of errors, but it was not until the early century that it was discovered that these errors followed a normal distribution. Independently, the mathematicians Adrain in and Gauss in developed the formula for the normal distribution and showed that errors were fit well by this distribution.\nThis same distribution had been discovered by Laplace in when he derived the extremely important central limit theorem, the topic of a later section of this chapter. Laplace showed that even if a distribution is not normally distributed, the means of repeated samples from the distribution would be very nearly normally distributed, and that the larger the sample size, the closer the distribution of means would be to a normal distribution.\nQuételet was the first to apply the normal distribution to human characteristics. He noted that characteristics such as height, weight, and strength were normally distributed."
  },
  {
    "objectID": "posts/3. UND/UND.html#probability-density-function-pdf",
    "href": "posts/3. UND/UND.html#probability-density-function-pdf",
    "title": "Univariate Normal Distributions",
    "section": "Probability Density Function (PDF)",
    "text": "Probability Density Function (PDF)\n\nNormal Distribution\nA random variable X is normally distributed with mean and variance if it has the probability density function of X as:\n\\[\nf(x; \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}exp{[-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2}]}\n\\] \\[, -\\infty &lt; x &lt; \\infty , -\\infty &lt; \\mu &lt; \\infty , \\sigma &gt; 0 \\]\nThe constants \\(\\mu\\), \\(\\sigma\\), and \\(\\sigma^2\\) are, respectively, the mean, standard deviation, and variance of the normal distribution.\ne = natural logarithm \\(\\pi\\) = constant pi\n\n\nStandard Normal Distribution\nA normal distribution with a mean of and a standard deviation of is called a standard normal distribution.\nLet F(x;\\(\\mu\\), \\(\\sigma^2\\)) denote the cumulative distribution function (cdf) of X.\nThe pdf of a standard normal random variable Z is given by\n\\[\n\\phi(z) = \\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{z^{2}}{2})  \n\\] \\[ ,  -\\infty &lt; z &lt; \\infty \\]\nLet \\(\\Phi(z)\\) denote the cdf of z.\nThroughout the book the notation \\(N(\\mu, \\sigma^2)\\) will denote the normal distribution \\(F(\\cdot, \\mu, \\sigma^2)\\) with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Also, the notations f, F, \\(\\phi, and \\Phi\\) are used only in reference to normal distributions"
  },
  {
    "objectID": "posts/3. UND/UND.html#cumulative-distribution-function-cdf",
    "href": "posts/3. UND/UND.html#cumulative-distribution-function-cdf",
    "title": "Univariate Normal Distributions",
    "section": "Cumulative Distribution Function (CDF)",
    "text": "Cumulative Distribution Function (CDF)\nThe Cumulative Distribution Function (CDF) of a univariate normal distribution gives the probability that a normally distributed random variable𝑋takes on a value less than or equal to some value 𝑥.\n\nNormal Distribution\nFor a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) the CDF is expressed as:\n\\[F(x) = P(X\\le x) = \\int_{-\\infty }^{x}\\frac{1}{\\sigma\\sqrt{2\\pi}}exp(-\\frac{(t-\\mu)^{2}}{2\\sigma^{2}})dt\\]\n\n\nStandard Normal Distribution\nIn the case of the standard normal distribution, where \\(\\mu\\) = 0 and \\(\\sigma\\)=1 the CDF simplifies to:\n\\[\\Phi(x) = \\int_{-\\infty }^{x}\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{t^{2}}{2})dt\\]"
  },
  {
    "objectID": "posts/3. UND/UND.html#moment-generating-function-mgf",
    "href": "posts/3. UND/UND.html#moment-generating-function-mgf",
    "title": "Univariate Normal Distributions",
    "section": "Moment Generating Function (MGF)",
    "text": "Moment Generating Function (MGF)\nThe Moment Generating Function (MGF) of a univariate normal distribution provides a way to capture all moments of the distribution. For a random variable 𝑋that follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) the MGF is given by\n\\[M_{x}(t) = \\mathbb{E}[e^{tX}]\\]\nThe MGF for a normal distribution 𝑋∼𝑁(\\(\\mu\\),\\(\\sigma^2\\)) is:\n\\[M_{x}(t) = exp(\\mu t+\\frac{\\sigma^2 t^2}{2})\\]"
  },
  {
    "objectID": "posts/3. UND/UND.html#features",
    "href": "posts/3. UND/UND.html#features",
    "title": "Univariate Normal Distributions",
    "section": "Features",
    "text": "Features\n\nNormal distributions are symmetric around their mean\nThe mean, median, and mode of a normal distribution are equal.\nThe area under the normal curve is equal to .\nNormal distributions are denser in the center and less dense in the tails.\nNormal distributions are defined by two parameters, the mean (\\(\\mu\\)) and the standard deviation (\\(\\sigma\\)).\n68% of the area of a normal distribution is within one standard deviation of the mean.\nApproximately 95% of the area of a normal distribution is within two standard deviations of the mean."
  },
  {
    "objectID": "posts/6. MLE of Mixture UND/MLE of Mixture UNDs.html",
    "href": "posts/6. MLE of Mixture UND/MLE of Mixture UNDs.html",
    "title": "Maximum Likelihood Estimation of Mixture Univariate Normal Distributions",
    "section": "",
    "text": "MAXIMUM LIKELIHOOD EXTIMATION OF MIXTURE OF UNIVARIATE NORMAL DISTRIBUTION\nGaussian Mixture Model (GMM)\nMLE of Gaussian Mixture Model (GMM)\nGaussian Mixture Model is based on the assumption that the data is generated from a mixture of several Gaussian (normal) distributions. Each Gaussian in the mixture represents a cluster or subpopulation in the data.\nFor a Gaussian Mixture Model (GMM) with \\(K\\) components, the probability density function (pdf) of a data point \\(x\\) is:\n\\[\np(x|\\Theta) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)\n\\]\nwhere:\n\\(\\pi_k\\) - is the mixing weight of the \\(k\\)-th component, with \\(\\pi_k\\geq0\\) and \\((\\sum_{k=1}^K \\pi_k=1)\\) .\n\\(\\mathcal{N}(x|\\mu_k, \\Sigma_k))\\) - is the Gaussian pdf for the \\(k\\)-th component:\n\\[\n\\mathcal{N}(x \\mid \\mu_k, \\Sigma_k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}} \\exp\\left(-\\frac{1}{2} (x - \\mu_k)^T \\Sigma_k^{-1} (x - \\mu_k)\\right)\n\\]\n\\(K\\) is the total number of Gaussian Components.\nNote:\n\\(\\mu_k\\) - is the mean vector of the \\(k\\) - th Gaussian.\n\\(\\sum_k\\) - is the covariance matrix of the \\(k\\) - th Gaussian\nLikelihood Function\nGiven a dataset \\({x_1,x_2,\\ldots,x_n}\\), the likelihood function \\(\\mathcal{L}(\\theta)\\) is the joint probability of observing the entire dataset:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N p(x_i|\\Theta)\n\\]\nSubstituting the pdf of the GMM we have:\n\\[\n\\mathcal{L}(\\Theta) = \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nLog-Likelihood Function\nTo simplify the optimization process, we use the log-likelihood function, which is the logarithm of the likelihood function. Taking the logarithm we have:\n\\[\n\\mathcal{L}(\\Theta) = \\log \\left( \\prod_{i=1}^N \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right) \\right)\n\\]\nUsing the property of logarithms \\((\\log(ab)=\\log(a)+\\log(b))\\):\n\\[\n\\mathcal{L}(\\Theta) = \\sum_{i=1}^N \\log \\left( \\sum_{k=1}^K \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n\\]\nExpectation-Maximization (EM) Algorithm\nTo find the MLE of the parameters (means, covariances, and mixing weights) that maximize the likelihood of the observed data we use the Expectation-Maximization (EM) algorithm. The EM algorithm iterates between two steps:\nE-Step (Expectation Step)\nThe E-step computes the responsibility \\(\\gamma_{ik}\\)​, which is the probability that data point \\(\\mathbf{x}_k\\)​ was generated by the \\(k\\)-th Gaussian component. Mathematically, the responsibility is given by the conditional probability:\n\\[\n\\gamma_{ik} = P(z_i = k|\\mathbf{x}_i)\n\\]\nwhere:\n\\(z_i\\) is the latent variable indicating the component responsible for generating \\(\\mathbf{x}_i\\),\n\\(P(z_i=k|\\mathbf{x}_i)\\) is the posterior probability that component \\(k\\) is responsible for \\(\\mathbf{x}_i\\)​.\nBy Bayes’ theorem, the responsibility \\(\\gamma_{ik}\\)​ can be written as:\n\\[\n\\gamma_{ik} =\\frac{P(z_i=k)P(\\mathbf{x}_i|z_i=k)}{P(\\mathbf{x}_i)}\n\\]​\nWhere:\n\\(P(z_i=k)=\\pi_k\\) is the prior probability of selecting the kkk-th component (the mixing weight),\n\\(P(\\mathbf{x}i|z_i=k)=\\mathcal{N}(\\mathbf{x}i|\\mu_k, \\Sigma_k)\\) is the likelihood of the data point \\(\\mathbf{x}_i\\) under component \\(k\\),\n\\(P(\\mathbf{x}_i)\\) is the marginal likelihood, or the total probability of observing \\(\\mathbf{x}_i\\)​, which is a weighted sum of the likelihoods over all components.\nThen expressing the marginal likelihood \\(P(\\mathbf{x}_i)\\) as:\n\\[\nP(\\mathbf{x}_i) = \\Sigma_{j=1}^K P(z_i=j)P(\\mathbf{x}_i|z_i=j)\n\\]\nand the Gaussian likelihoods, we get:\n\\[\nP(\\mathbf{x}_i)=\\Sigma_{j=1}^K \\pi_j\\mathcal{N}(\\mathbf{x}_i|\\mu_j, \\Sigma_j)\n\\]\nNow, we substitute the expressions for \\(P(z_i=k), P(\\mathbf{x}i|zi=k)\\) , and \\(P(\\mathbf{x}_i)\\) into the equation for \\(\\gamma_{ik}\\)​:\n\\[\n\\gamma_{ik} =\\frac{\\pi_k\\mathcal{N}(\\mathbf{x}_i|\\mu_k, \\Sigma_k)}{\\Sigma_{j=1}^K\\mathcal{N}(\\mathbf{x}_i|\\mu_i,\\Sigma_j)}\n\\]\nHence, this is the responsibility \\(\\gamma_{ik}\\)​, which represents the posterior probability that data point \\(\\mathbf{x}_i\\)​ was generated by component \\(k\\).\nM-Step (Maximization Step)\nUpdating the parameters \\((\\pi_k)\\), \\((\\mu_k)\\), and \\((\\Sigma_k)\\) using the responsibilities \\((\\gamma_{ik})\\) :\nUpdate Mixing Weights\nThe mixing weight \\((\\pi_k)\\) is updated as:\n\\[\n\\pi_k = \\frac{1}{N} \\sum_{i=1}^N \\gamma_{ik}\n\\]\nIn a GMM, the data is assumed to be generated from a mixture of \\(K\\) Gaussian distributions. Each Gaussian component \\(k\\) has an associated mixing weight \\(\\pi_k\\)​. These weights determine how much each Gaussian component contributes to the overall mixture model.\nMathematical Representation: The mixing weight \\(\\pi_k\\) is a non-negative value that satisfies:\n\\[\n\\pi_k\\geq0\n\\]\nand\n\\[\n\\sum_{k-1}^K \\pi_k=1\n\\]\nThis ensures that the weights are properly normalized to sum to 1, representing a probability distribution over the components.\nUpdate Means\nThe mean \\((\\mu_k)\\) is updated as:\n\\[\n\\mu_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} x_i}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nwhere:\n\\(\\gamma_{ik}\\) is the responsibility of component \\(k\\) for data point \\(x_i\\)​, calculated in the E-step.\nThe numerator is the weighted sum of the data points, and the denominator is the sum of the responsibilities for component \\(k\\).\nUpdate Covariances\nThe covariance \\((\\Sigma_k)\\) is updated as:\n\\[\n\\Sigma_k = \\frac{\\sum_{i=1}^N \\gamma_{ik} (x_i - \\mu_k) (x_i - \\mu_k)^T}{\\sum_{i=1}^N \\gamma_{ik}}\n\\]\nWhere:\n\\(\\gamma_{ik}\\) is the responsibility that component \\(k\\) has for data point \\(x_i\\)​, calculated in the E-step.\n\\((x_i - \\mu_k)\\) is the difference between the data point and the mean of the kkk-th component.\nThe numerator represents the weighted sum of the outer products of the difference vectors \\((x_i - \\mu_k)\\), which captures both the variance and covariance of the data assigned to the \\(k\\)-th component.\nThe denominator is the sum of the responsibilities for component \\(k\\), which normalizes the weighted sum.\nIn conclusion, to get the MLE of the Gaussian Mixture Model we use the Expectation-Maximization Algorithm. This algorithm is particularly useful for GMMs because it can handle the uncertainty about which Gaussian Component generated each data point, leading to soft clustering and more flexible modeling of complex data distributions."
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html",
    "href": "posts/Paper Proper/Thesis Paper.html",
    "title": "Thesis Proper",
    "section": "",
    "text": "This chapter provides an overview of Gaussian Mixture Models (GMMs), focusing on their theoretical framework and practical applications. It emphasizes the importance of GMMs in pattern recognition, and in identifying structures within complex datasets. This introduction offers valuable insights into how GMMs can be effectively used across various fields by exploring their concepts and application to real-world settings.\n\n\nIn recent years, machine learning has significantly progressed in its capacity to model intricate data. Numerous approaches have been created, including the Gaussian Mixture Model (GMM), which has been particularly notable for its flexibility and effectiveness in representing various data distributions. The Gaussian Mixture Model (GMM) is a probabilistic model representing a distribution as a combination of multiple Gaussian distributions or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM is exceptionally flexible in modeling complex, multimodal data distributions (Carreira-Perpinan, 2000), making them ideal for clustering, density estimation, and pattern recognition applications.\nAs noted by Titterington et al. (1985), Gaussian Mixture Models are ubiquitous probabilistic model for density estimation in machine learning. For instance, they are crucial in classification, such as handwritten digit recognition, where they model pixel distributions for each digit, ensuring accurate and robust classification. Additionally, GMMs play a significant role in dimensionality reduction techniques such as Generative Topographic Mapping (GTM), enabling visualization of high-dimensional data in lower-dimensional spaces.\nMoreover, according to Bishop (2006) and Carreira-Perpinan (2000), GMMs are extensively used in data mining, pattern recognition, machine learning and statistical analysis effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components.\nThe increasing use of GMMs has led to the development of efficient algorithms that not only estimate model parameters but also accurately recognize essential aspects, such as the mixture’s modes (peaks). Identifying these modes can provide more information about the structure of the data and improve model interpretability, especially in complicated multi-modal datasets. Current optimization approaches, such as Newton’s Method and Gradient Ascent, are helpful but require additional development for practical use.\n\n\n\n[wala pay sulod]\n\n\n\nThis study aims to comprehensively analyze Gaussian Mixture Models (GMMs) from theoretical and practical perspective. The objectives of the study are as follows:\n\nExplore the theoretical properties of Gaussian Mixture Models, including their definitions, properties, and methods for parameter estimation.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for Gaussian Mixture Model, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMMs in classification problems, pattern recognition, and Generative Topographic Mapping, examining how GMM can contribute to this\nDevelop an algorithm for efficiently locating a given GMM’s modes\nImplement Newton’s Method and Gradient Ascent for mode detection\nUtilize the Hessian Matrix to calculate local confidence intervals around the modes\nExplore the potential of Smart Entropy in enhancing the performance of the mode detection algorithm\n\n\n\n\n[wala pay sulod]\n\n\n\nThis study focuses on developing an algorithm for locating modes in Gaussian Mixture Models (GMMs). The theoretical scope will thoroughly examine GMMs, including their definitions, properties, and parameter estimate techniques. The study will additionally look into Maximum Likelihood Estimation and the Expectation-Maximization Algorithm, which are essential for estimating GMM parameters.\nIn terms of application, the study will primarily focus on two main areas: problems with classification (mainly handwritten digit recognition) and Generative Topographic Mapping for dimension reduction. However, this research is limited by numerous factors:\n\nThe application examples, such as handwritten digit recognition, will be based on publicly available datasets, particularly those mentioned in (Bishop, 2006, Pattern Recognition and Machine Learning).\nThe optimization methods investigated (Newton’s Method, Gradient Ascent, etc.) may be computationally expensive, and their performance depends on the computer resources available.\nThe problem of converging to local optima remains a restriction, particularly in extremely complex or multimodal data sets.\n\n\n\n\n[walay sulod]"
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html#background-of-the-study",
    "href": "posts/Paper Proper/Thesis Paper.html#background-of-the-study",
    "title": "Thesis Proper",
    "section": "",
    "text": "In recent years, machine learning has significantly progressed in its capacity to model intricate data. Numerous approaches have been created, including the Gaussian Mixture Model (GMM), which has been particularly notable for its flexibility and effectiveness in representing various data distributions. The Gaussian Mixture Model (GMM) is a probabilistic model representing a distribution as a combination of multiple Gaussian distributions or components. Each component in the mixture is defined by its mean, covariance, and mixing coefficient. GMM is exceptionally flexible in modeling complex, multimodal data distributions (Carreira-Perpinan, 2000), making them ideal for clustering, density estimation, and pattern recognition applications.\nAs noted by Titterington et al. (1985), Gaussian Mixture Models are ubiquitous probabilistic model for density estimation in machine learning. For instance, they are crucial in classification, such as handwritten digit recognition, where they model pixel distributions for each digit, ensuring accurate and robust classification. Additionally, GMMs play a significant role in dimensionality reduction techniques such as Generative Topographic Mapping (GTM), enabling visualization of high-dimensional data in lower-dimensional spaces.\nMoreover, according to Bishop (2006) and Carreira-Perpinan (2000), GMMs are extensively used in data mining, pattern recognition, machine learning and statistical analysis effectively capture the underlying structure of data by representing different clusters or features through distinct Gaussian components.\nThe increasing use of GMMs has led to the development of efficient algorithms that not only estimate model parameters but also accurately recognize essential aspects, such as the mixture’s modes (peaks). Identifying these modes can provide more information about the structure of the data and improve model interpretability, especially in complicated multi-modal datasets. Current optimization approaches, such as Newton’s Method and Gradient Ascent, are helpful but require additional development for practical use."
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html#statement-of-the-problem",
    "href": "posts/Paper Proper/Thesis Paper.html#statement-of-the-problem",
    "title": "Thesis Proper",
    "section": "",
    "text": "[wala pay sulod]"
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html#objectives-of-the-study",
    "href": "posts/Paper Proper/Thesis Paper.html#objectives-of-the-study",
    "title": "Thesis Proper",
    "section": "",
    "text": "This study aims to comprehensively analyze Gaussian Mixture Models (GMMs) from theoretical and practical perspective. The objectives of the study are as follows:\n\nExplore the theoretical properties of Gaussian Mixture Models, including their definitions, properties, and methods for parameter estimation.\nExamine the role of the Expectation-Maximization (EM) algorithm in Maximum Likelihood Estimation (MLE) for Gaussian Mixture Model, including its convergence properties, potential limitations, and ways to optimize its performance for large or high-dimensional datasets.\nInvestigate the application of GMMs in classification problems, pattern recognition, and Generative Topographic Mapping, examining how GMM can contribute to this\nDevelop an algorithm for efficiently locating a given GMM’s modes\nImplement Newton’s Method and Gradient Ascent for mode detection\nUtilize the Hessian Matrix to calculate local confidence intervals around the modes\nExplore the potential of Smart Entropy in enhancing the performance of the mode detection algorithm"
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html#significance-of-the-study",
    "href": "posts/Paper Proper/Thesis Paper.html#significance-of-the-study",
    "title": "Thesis Proper",
    "section": "",
    "text": "[wala pay sulod]"
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html#scope-and-limitation",
    "href": "posts/Paper Proper/Thesis Paper.html#scope-and-limitation",
    "title": "Thesis Proper",
    "section": "",
    "text": "This study focuses on developing an algorithm for locating modes in Gaussian Mixture Models (GMMs). The theoretical scope will thoroughly examine GMMs, including their definitions, properties, and parameter estimate techniques. The study will additionally look into Maximum Likelihood Estimation and the Expectation-Maximization Algorithm, which are essential for estimating GMM parameters.\nIn terms of application, the study will primarily focus on two main areas: problems with classification (mainly handwritten digit recognition) and Generative Topographic Mapping for dimension reduction. However, this research is limited by numerous factors:\n\nThe application examples, such as handwritten digit recognition, will be based on publicly available datasets, particularly those mentioned in (Bishop, 2006, Pattern Recognition and Machine Learning).\nThe optimization methods investigated (Newton’s Method, Gradient Ascent, etc.) may be computationally expensive, and their performance depends on the computer resources available.\nThe problem of converging to local optima remains a restriction, particularly in extremely complex or multimodal data sets."
  },
  {
    "objectID": "posts/Paper Proper/Thesis Paper.html#methodology",
    "href": "posts/Paper Proper/Thesis Paper.html#methodology",
    "title": "Thesis Proper",
    "section": "",
    "text": "[walay sulod]"
  }
]